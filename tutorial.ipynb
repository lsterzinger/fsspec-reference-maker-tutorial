{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Kerchunk` tutorial\n",
    "\n",
    "Created July 2021 by [Lucas Sterzinger](mailto:lsterzinger@ucdavis.edu) ([Twitter](https://twitter.com/lucassterzinger)) as part of the NCAR [Summer Internship in Parallel Computational Science (SIParCS)](https://www2.cisl.ucar.edu/siparcs)\n",
    "\n",
    "If any part of this tutorial is now out of date, please feel free to open a pull request with a fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ReferenceMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerchunk.hdf import SingleHdf5ToZarr \n",
    "from kerchunk.combine import MultiZarrToZarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import s3fs\n",
    "import datetime as dt\n",
    "import zipfile\n",
    "import logging\n",
    "import fsspec\n",
    "import ujson\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask makes some of the processing faster, but it is not required to run this tutorial.\n",
    "This code block starts 8 local dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=8)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata JSONs\n",
    "\n",
    "### Get a list of S3 files for the 210th day of 2020 (July 28, 2020) and prepend `s3://` to each url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)\n",
    "\n",
    "urls = ['s3://' + f for f in fs.glob(\"s3://noaa-goes16/ABI-L2-SSTF/2020/210/*/*.nc\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reference JSONS\n",
    "### This function creates JSON metadata files for each of the S3 files in the local `jsons/` directory\n",
    "\n",
    "These files point to the S3 location of the netCDF files, and only need to be created once. Tihs process took me about 10 minutes to generate the JSONs for 24 files. This function could easily be made to run in parallel for faster performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(u):\n",
    "    so = dict(\n",
    "        mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"none\"\n",
    "    )\n",
    "    with fsspec.open(u, **so) as inf:\n",
    "        h5chunks = SingleHdf5ToZarr(inf, u, inline_threshold=300)\n",
    "        with open(f\"jsons/{u.split('/')[-1]}.json\", 'wb') as outf:\n",
    "            outf.write(ujson.dumps(h5chunks.translate()).encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json/ folder if it doesn't already exist\n",
    "import pathlib\n",
    "pathlib.Path('./jsons/').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `gen_json()` function defined above with dask. This function can be run in parallel with each worker creating a single file. If you do not want to use dask, replace the uncommented line with the commented block below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(*[dask.delayed(gen_json)(u) for u in urls]);\n",
    "\n",
    "# If not using dask, use\n",
    "# for u in tqdm(urls):\n",
    "#     gen_json(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Read remote netCDF files with xarray and fsspec\n",
    "\n",
    "### First, create a list of JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = sorted(glob(\"./jsons/*.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, loop over the files and use `fsspec.get_mapper()` to create mappers for each file object, creating a list of mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = []\n",
    "for j in tqdm(json_list):\n",
    "    with open(j) as f:\n",
    "        m_list.append(fsspec.get_mapper(\"reference://\", \n",
    "                        fo=ujson.load(f),\n",
    "                        remote_protocol='s3',\n",
    "                        remote_options={'anon':True}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the mapper list can be passed directly to xarray.open_mfdataset() as long as the engine is specified as \"zarr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(m_list, engine='zarr', combine='nested', concat_dim='t', \n",
    "                        coords='minimal', data_vars='minimal', compat='override',\n",
    "                        parallel=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One JSON, multiple data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1-file, 1-JSON method above doesn't scale well for larger datasets.\n",
    "Instead, we can combine multiple JSONS into a single JSON describing the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    json_list,\n",
    "    remote_protocol=\"s3\",\n",
    "    remote_options={'anon':True},\n",
    "    xarray_open_kwargs={\n",
    "        \"decode_cf\" : False,\n",
    "        \"mask_and_scale\" : False,\n",
    "        \"decode_times\" : False,\n",
    "        \"decode_timedelta\" : False,\n",
    "        \"use_cftime\" : False,\n",
    "        \"decode_coords\" : False\n",
    "    },\n",
    "    xarray_concat_args={\n",
    "        'data_vars' : 'minimal',\n",
    "        'coords' : 'minimal',\n",
    "        'compat' : 'override',\n",
    "        'join' : 'override', \n",
    "        'combine_attrs' : 'override',\n",
    "        'dim' : 't'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part actually writes the metadata to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mzz.translate(\"combined.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use the `combined.json` file to open the multifile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs = fsspec.filesystem(\"reference\", fo=\"./combined.json\", remote_protocol=\"s3\", \n",
    "                        remote_options={\"anon\":True}, skip_instance_cache=True)\n",
    "m = fs.get_mapper(\"\")\n",
    "ds = xr.open_dataset(m, engine='zarr')\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a subset of the data (in this case, the Gulf Stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a single time with `.isel(t=14)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subset = ds.sel(x=slice(-0.01,0.07215601),y=slice(0.12,0.09))  #reduce to GS region\n",
    "\n",
    "masked = subset.SST.where(subset.DQF==0)\n",
    "\n",
    "masked.isel(t=14).plot(vmin=14+273.15,vmax=30+273.15,cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a mean along the time axis (1-day average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "subset = ds.sel(x=slice(-0.01,0.07215601),y=slice(0.12,0.09))  #reduce to GS region\n",
    "\n",
    "masked = subset.SST.where(subset.DQF==0)\n",
    "\n",
    "masked.mean(\"t\", skipna=True).plot(vmin=14+273.15,vmax=30+273.15,cmap='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For details on how to plot GOES data on a lat/lon grid, see [this blog post I wrote](https://lsterzinger.medium.com/add-lat-lon-coordinates-to-goes-16-goes-17-l2-data-and-plot-with-cartopy-27f07879157f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3bd1516611ce5bb0d255a0db4dd9a6f38a599d9a5d246e2fe5677e0b16b804b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('fsspec-tutorial': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
